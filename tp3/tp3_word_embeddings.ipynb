{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 : Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import sys\n",
    "import os\n",
    "\n",
    "import pickle\n",
    "import nltk\n",
    "from gensim.models import Word2Vec\n",
    "from gensim.models.phrases import Phraser, Phrases\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode\n",
    "\n",
    "# config\n",
    "\n",
    "temp_path = '../data/tmp/'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un objet qui streame les lignes d'un fichier pour économiser de la RAM\n",
    "\n",
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\" #permet de tokenizer des \"doubles\" mots, par ex \"premier\" + \"ministre\" va être considéré comme un token \"premier ministre\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]\n",
    "\n",
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet 'phrases' = \"dictionnaire d'expressions multi-mots associées à un score\", dont les clés correspondent aux termes du corpus\n",
    "\n",
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des objets 'phrases' en objet 'phraser' = version light du 'phrases' -> convertit certains unigrams en bigrams s'ils sont pertinents\n",
    "\n",
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)\n",
    "\n",
    "# Sauvegarder l'objet Phraser des bigrammes\n",
    "\n",
    "with open(f\"{temp_path}bigrams.p\", 'wb') as f:\n",
    "    pickle.dump(bigram_phraser, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ouvrir l'objet Phraser des bigrammes\n",
    "\n",
    "bigram_phraser = pickle.load(open(f\"{temp_path}bigrams.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion en objet Phraser et sauvegarde du Phraser des trigrammes\n",
    "\n",
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)\n",
    "\n",
    "with open(f\"{temp_path}trigrams.p\", 'wb') as f:\n",
    "    pickle.dump(trigram_phraser, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créations d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "bigram_phraser = pickle.load(open(f\"{temp_path}bigrams.p\", \"rb\"))\n",
    "trigram_phraser = pickle.load(open(f\"{temp_path}trigrams.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[['mi', 'imnri', 'r', 'i', '<<', 'i', 'i', 'hmu', \"'\", 'i', '/', 'tx', \"-'\", 'l', ':', 'marche', 'tenu', 'hors', 'villa', ',', 'la', '9', '.'], ['--', 'u', 'a', 'ete', 'vaain', 'si', 'teicj', '>>', 'm', 'races_indigenes', 'de', 'fr', '.'], ['31', '<)', 'a', '5s', \"'\", 'k', '131', 'de', '.'], ['rasa', 'iichakdui', \"'\", 'te', ',', 'do', '(', 'r', '.', '3s0', 'h', '710', '.', 'taureaux', 'iallsenas', ',>', 'ia', 'u', '\\\\', '--', 'a', '--', ';', '0ii', '.'], ['hollandais', ',', 'dufr', '.'], ['0', '.'], ['--', 'a', '9', '.--', 'la', 'idto', '-', 'vachei', 'laitieres', ':', 'bn', 'vante', '1q', '.'], ['vendues', '3', '\\\\', 'au', 'prix', 'la', '410', 'a', '*', '<<', 'i', 'h', '\\\\;', 'genisses', ',', 'kl', '.'], [\"'.\", '9', '.'], ['i', 'l', '.', '2', 'i', '.', 'id', '.'], ['da', '370', 'i', '6lutr', '.'], ['marche', 'a', '<', 'u', 'porcs', '.'], ['--', 'categorie', 'de', 'lt', 'ilashtya', ':', \"'\", '237', 'on', 'vente', ';', 'vendus', '1', 'm', '.', 'do', \"'\", '2', 'i', '.--', 'a', ';:,', 'l', '--;', 'i', '.', 'l', '.'], ['des', 't', \"'\", 'innlrov', '-', 'i3ie', ';>>', 'vente', ',', 'vendus', '93', '.', 'de', '32', '.--', 'a', '52', '.--.'], ['xumiir', '.'], ['10', 'a', '*', 'v', '.--_froment', '>>', 'las', '190', 'kit', '..', 'fr', '.'], ['15', '.', '23', ';', 'mi', \"'\", 'te', '>>,', '--;', 'epautre', '.', 'l3', '.--', ';', 'seigle', '.'], ['14', '.', '5', '>;', 'avoine', '.'], ['10', '.', '30', ';', 'orge', '.'], ['--;_feveroles', ',', '22', '.--;_pommes', 'do', 'terre', '.'], ['g', '.--;_paille', ',', 'v', '.', '--;', 'fcin', '.', '6', '.', '50', ':', 'lioumon', ',', '--.'], ['2', \"%'\", 'ivellom', '.'], ['10', 'mv', '.'], ['--', 'froment', ',', 'las', '109_kll', '.,', '(', 'r', '.', '13', '.'], ['--', '\\\\', '--;_selgle', '.', 'ii', '.-;\\\\', '--;_avoine', ',', '16', '.', '509', '-;', 'orge', ',', '17', '30', 'j', '--,--;_escourgeon', '.'], ['#', 'inint', '-', 't', '>>*', 'o', '<<', 'd', ',', '10', 'nov', '.', '--', 'froment', 'je', 'culture', ',', 'par', '400kil', '.,', 'fr', '.'], ['10', '.--', '1', '--.--;', 'm', '.', 'commercial', ',', '16', '.'], ['--', 'a', '--.--,_seigle', '.'], ['13', '.', '50', 'a', '--;_avoine', '.'], ['10', '.', '50', 'a', '--.--;_orge', '.'], ['17', '.--', 'a', '--;_farine', 'ira', 'de', 'froment', ',', '0', '.--', 'a', '0', '.'], ['--;', 'son', 'gros', 'de', 'tco', '*', 'ment', ',', '15', '.--', 'a', '--.--.'], ['huiie', 'de', 'colza', ',', 'par', 'hcctoi', '.,', '--', 'a', '--;', 'u', '.', 'epures', ',', '--~', 'a', ';', 'huile', 'de', 'lin', '.'], ['--,--', 'a', '--;_tourteaux', 'de', 'colza', 'par', '100_kll', '.,', '--', 'a', '--;_id', '.'], ['de', 'lin', ',', '--', 'a', 'i', 'pommes', 'de', 'terre_blanches', ',', '6', '.', '3', '>>', 'a', '0', '.--.'], ['...', ',,', ',,,,', 'paille', ',', 'fi', '.', '50', 'a', '0', '.--;', 'loln', \",'\", '5', '.', '5i', ')', 'a', '0', '.--;_beurre', ',', 'le', 'kilo', '.'], ['2', '.', 'o', 'a', '0', '.--;_oeufs', ',', 'les', '26', ',', '3', '.', '30', 'a', '0', '.--;_genievre', ',', 'l', \"'\", 'uect', '.,', '--', 'a', 'esprit', ',', '0', '.--', 'a', '0', '.--.'], ['tournai', ',', '10', 'nov', '.', '--', 'froment_blaze', ',', 'l', \"'\", 'iiaetoluri', ',', 'fr', '.'], ['--.--', 'a', '--;_froment', ',--', 'u', '--;_metell', ',', '--', 'a', '--;_seigle', ',--.--', 'a', '--;_avoine', ',', '--', 'a', '--;', 'feve', '-', 'roles', ',', '--.--', 'a', '--;_beurre', ',', 'le', 'kilo', ',', '2', '.', '00', 'a', '3', ',', '10', ';', 'oeufs', ',', 'les', '26', ',', '3', '.', '70', 'u3', '.', 'c0', '.'], ['wacrofiliom', ',', '10', 'nov', '.--', 'beurre', '.', 'le', '1', '/', '3', 'il', '-,', '1', '.', '5qa', '1', '.', '60', ',', 'oeufs', ',', 'les', '26', ',', '3', '.', '99', 'a', '1', '.', '16', ';', 'lin', ',', 'les', '100_kilos', ',', '93', '.--', 'a', '145', '.--;_etoupes', ',', 'lee', '1u0', 'kii', '.,', '30', '.--', 'a', '35', '.--;_pommes', 'de', 'terre', ',', 'les', '100', '>', 'il', '.,', '3', '.--', 'a', '3', '.', '50', ';', 'jeune', 'porc', ',', 'la', 'piece', ',', '33', '.--', 'a', '50', '.--;_lapin', '.'], ['2', '.--', 'a', '3', '.', '95', ';', 'poulet', ',', '2', '.', '25', 'a', '3', '.'], ['--.'], ['vin', '-', 'e', '<<,', '10', 'nov', '.', '--', 'froment', ',', 'les', '10j', 'kll', '.,', 'fr', '.'], ['18', '.--', 'a', '19', '.', '59', ';', 'seigle', ',', '16', '.-', 'a', '16', '.', '75', ';', 'avoine', ',', '16', '.--', 'a', '16', '.', '75', ':', 'pois', '--', 'a', '--;_feveroles', ',', '--.--', 'a', '--.--;_pommes', 'de', 'terre', ',', '8', '80', 'a', 'g', '.--*,', 'beurre', ',', 'le', 'kilo', ',', '2', '.', '90', 'a', '3', '.', '20', ';', 'oeufs', ',', 'le', '(', 'iuart3', '-', 'ron', '.'], ['i', '.', '16', 'a', '4', '.', '63', '.'], ['charmanne', '.'], ['consul_general', 'de', 'belgique', 'a', 'ottawa', '(', 'canada', ').', 'a', 'ele', 'nomme_consul', 'general', 'a', 'bang_kok', '(', 'siam', '),', 'avec', 'juridiction', 'sur', 'le', 'siam', 'et', 'les', 'slrails', 'seluements', '.'], ['--', 'm', '.', 'kelels', ',', 'consul', 'de', 'belgique', 'a', 'tien', '-', 'tsin', '(', 'chine', '),', 'a', 'ele', 'nomme_consul', 'a', 'ottawa', '(', 'canada', '),', 'avec', 'juridiction', 'sur', 'la', 'federation', 'canadienne', 'et', 'ja', 'colonie', 'de', 'terre', '-', 'neuve', '.'], ['decoration', 'c', \".'\", 'v', \"'\", 'gae', '.'], ['--', 'la', '.', 'decoration_civique', 'a', 'ele', 'decernee', 'aux', 'agents', 'de', 'l', \"'\", 'administration', 'des', 'postes', ';', 'designes_ci', '-', 'apres', ',', 'savoir', ':', 'la', 'medaille', 'de', 'ire_classe', ':', 'a', 'm', '.', 'billy', '.'], ['facteur_rural', '.'], ['la', 'medaille', 'de', '2e_classe', ':', 'a', 'm', '.', 'goringy', 'facteur', 'local', '.', '.', 'armee', '.'], ['--', 'le', 'sergent', ',', 'en', 'conge_illimite', ',', 'baudouin', '\"', 'est', 'nomme', 'sous', '-', 'lieutenanl', 'payeur', 'de', 'reserve', '.'], ['notariat', '.'], ['--', 'sont_acceptees', 'les', 'demissions', 'de', 'm', '.', 'myin', '.'], ['de', 'ses_fonctions', 'de', 'notaire', 'a', 'la', 'residence', 'd', \"'\", 'anvers', ';', 'do', 'm', '.', 'jadol', ',', 'id', '.'], ['de', 'marche', '.'], ['ecole_militaire', '.'], ['--', 'sont', 'admis', 'a', 'l', \"'\", 'ecole_militaire', ',', 'en', 'qualite', 'd', \"'\", 'eleves', 'de', 'la', '57e', 'promotion', 'de', 'l', \"'\", 'infanterie', 'et', 'ao', 'la', '<', \"'\", '*', \"'\", '*', '-', '\"', 'i', 'cavalerie', ',', 'les', 'jeunes_gens', 'dont', 'les', 'noms_suivent', ':', 'hanon', 'de', 'louvet', ',', 'van_sprang', ',', 'deyloo', ',', 'petit', ',', 'champagne', '.'], ['masui', ',', 'gerard', ',', 'vanneste', ',', 'lallemand', ',', 'van', 'iioeeke', \".'\", 'rigano', '.'], ['ilendrickx', ',', 'von', 'glaboke', ',', 'sou', '-', \"'\", 'moy', '.', 'couturieaux', ',', 'lucion', ',', 'mersch', ',', 'iledo', ',', 'iloudmont', ',', 'de', 'heusch', '.'], ['terfve', '.'], ['labio', ',', 'brabant', ',', 'franckx', ',', 'foulon', ',', 'fiahiiifirh', '.'], ['simmi', '\"', 'tfannatvnin', 'fivnv', '^', 'f', 'h', '-.', 'ilu', '/', 'phnin', '.'], ['u', 'gernaert', ',', 'simon', \",'\", 'henncquin', ',', 'fcro', \"'\", 't', ',', 'g', '->', 'ile', ',', 'i', \"'\", 'orjo', 'e', \"'\", 'y', 'hannus', '.', 'noel', ',', 'flanieng', ',', 'bri', '-', 'matchovelette', ',', 'gondry', ',', 'doux', ',', 'vermeuleu', ',', 'giilo', 'gillot', ',', 'boufvin', '.'], ['borremans', '.'], ['academie_royale_flamande', 'de', 'langue', 'et', 'de', 'litterature', '.'], ['--', 'l', \"'\", 'election', 'faite', 'par', 'l', \"'\", 'academie_flamande', ',', 'dans', 'sa', 'seance', 'du', '17', 'octobre_1906', ',', 'de', 'm', '.', 'le', 'docteur', 'hugo_verriest', ',', 'a', 'ingoyghem', ',', 'en', 'qualite', 'de', 'membre_effectif', ',', 'en', '-', 'remplacement', 'de', 'feu', 'm', '.', 'janssens', ',', 'est_approuvee', '.'], ['sapeurs', '-', 'pompiers_communaux', 'armes', '.'], ['--', 'm', '.', 'pa', '*', 'njels', ',', 'sous', '-', 'fieu', 'tenant', 'au', 'corps', 'arme', 'de', 'sapeurs', '-', 'pom', '-', 'iers', 'communaux', 'de', 'schaerheek', ',', 'est', 'nomme', 'lieule', '-', 'p', 'u', 'teuant', ',-', 'en_remplacement', 'de', 'm', '.', 'verteneuil', ',', 'decede', '.'], ['enseignement_moyen', '.'], ['--', 'mme', 'schaefer', '-', 'misonne', ',.'], ['.', 'directrice', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'elat', 'pour', 'alles', ',', 'a', 'jumet', ',', 'est', 'dechargee', 'des', 'fonciions', 'de', 'regente', 'd', \"'\", 'economie', 'domes', '!'], ['ique', 'a', 'l', \"'\", 'ecole', '\"', 'moyen', 'ne', 'del', \"'\", 'ktai', 'pour', 'ailes', ',', 'a', 'la', 'louviere', '.'], ['--', 'm', '.', 'barzin', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'regent', 'a', 'l', \"'\", 'ecole_moyenne', 'de', 'l', \"'\", 'etal', 'pour', \"'\", 'garcons', ',', 'a', 'spa', ',', 'avec', 'autorisation', 'd', \"'\", 'en', 'conserver', 'le', 'r', 'titre_honorifique', ':', 'il', 'est', 'admis', 'a', 'faire_valoir', 'ses_droits', ';', 'a', 'ja', 'pension', '.'], [';', \"'\", '-', 'r', '-', 'm', '.', 'drainer', ',', 'directeur', 'a', 'titre_provisoire', 'de', 'l', \"'\", 'ecole', 'moyenue', 'de', 'l', \"'\", 'etat', 'pour', '.', 'garcons', '^', 'a', 'wavre', ',', 'est', 'decharge', ',', 'sur', 'sa', 'demande', ',', 'des', 'fonctions', 'de', 'professeur', 'de', 'gymnastique', ',', 'en', ',', 'partage', ',', 'a', 'l', \"'\", 'ecole_moyenne', 'do', 'l', \"'\", 'etat', 'pour', 'garcons', ',', 'a', 'roeulx', '.'], ['*', 'contributions_directes', ',', 'douanes', 'et', 'accises', '.'], ['--', 'm', '.'], ['du', '-', '#', 'val', ',', 'receveur', 'des', 'contributions_directes', 'et', 'des', 'accises', 'a', 'deynze', '.', 'est', 'admis', ',', 'sur', 'sa', 'demande', ',', 'a', 'faire_valoir', 'ses_droits', 'a', 'la', 'pension', 'de', 'retraite', '.'], ['coinmission', 'tnedicale', 'provinciale', '.'], ['--', 'sont_nommes', 'membres_correspondants', 'de', 'la', 'commission_medicale', 'piminciale', 'de', 'namur', ';', 'mm', '.'], ['les', 'doctonrs', 'rolin', ',', 'u', \"'\", 'e', 'fosses', ';', 'lebrun', ',', 'de', 'ligny', ';', 'giliard', ',', 'do', 'ctiampion', ',', 'et', '.'], ['defasse', ',', 'de', 'spj', '\\\\', 'en_remplacement', 'de', 'mm', ',', 'les', 'docteurs', '.'], ['wery', ',', 'de', 'fosses', ',', 'et', 'fermine', ',', 'do', 'lignv', ',', 'demis', '-', ';', 'sionnairos', ',', 'et', 'de', 'mm', '.'], ['jes', 'docteurs', 'renard', ',', \"'\", 'de', 'champion', 'et', 'dehaybe', ',', 'de', 'spy', ',', 'decedes', '.'], ['arts', ',', 'sciences', 'et', 'lettres', 'declamation', '.', 'et', 'diction', '.'], ['*', '--', 'on', 'nous', 'demande', 'de', 'divers_cotes', 'des', 'renseignements', 'sur', 'le', 'cours', 'que', 'va', 'donner', 'm', '.', 'hittemans', '.'], ['nous', 'no', 'croyons_pouvoir', 'mieux', 'faire', 'que', 'd', \"'\", 'engager', 'les', 'interesses', 'a', 's', \"'\", 'adresser', 'a', 'l', \"'\", 'artiste', ',', 'rue', 'verhulst', ',', '6', ',', 'a', 'uccle', '.'], ['universite_libre', ',', 'rue', 'des', 'sols', '.'], ['--', 'demain_lundi', ',', 'a', 'r', 'meures', 'du', 'soir', ',', 'conference', 'par', 'm', '.', 'lameere', ':', '<<', 'la', 'fondation', '>>.'], ['universite_populaire', 'd', \"'\", 'euerbeek', '.'], ['--', 'lundi', '12', ',', 'a', '3', 'h', '.', '1', '/', '2', ',', 'au', 'local', ',', '4', ',', 'rue', 'do', 'l', \"'\", 'etang', ',', 'inauguration_solennelle', '.'], ['discours', 'de', 'm', '.', 'e', '.', 'richard', '.'], ['conference', 'de', 'xi', '.', 'ch', '.', 'buls', '.', 'sujet', ':', '<<-', 'la', 'corse', '>>.'], ['projections_lumineuses', '*', 'universite_populaire', 'de', 'saint', '-', 'josse', ',', '67', ',', 'rue', 'de', 'la', 'limite', '.'], ['demain_lundi', ',', 'a', '8', 'h', '.', '1', '/', '4', ',', 'eauserio', 'litteraire', ':', '<<', 'maxim', 'gorki', '<<.', 'lectures', '.'], ['vamicale', 'de', 'vecole', 'n_deg', '7', '.'], ['--', 'demain_lundi', ',', 'a', '8', 'h', '.', 'du', 'soir', ',', 'dans', 'le', 'preau', 'de', 'l', \"'\", 'ecole', 'n_deg', '7', ',', 'rue_haute', '225', \"'\", 'conference', 'par', 'madame', 'journaux', ':', '<<', 'de', 'la', 'grande', '-', 'chartreuse', 'a', 'ja', 'cote', 'd', \"'\", 'azur', '>>.', 'projections_lumineuses', '.'], ['foyer_intellectuel', ',', 's0', ',', 'rue', 'du', 'fort', '.'], ['--', 'demain_lundi', ',', 'a', '8b', ',,', 'm', '.', 'j', '.', 'vincent', ':', '<<', 'la', 'meteorologie', ':', 'le', 'barometre', '>>.']]\n"
     ]
    }
   ],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])\n",
    "\n",
    "# Imprimer une liste de n-grammes, qu'on répère car ils sont séparés par des _\n",
    "\n",
    "print(corpus[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sauvegarder cette liste dans le dossier temporaire\n",
    "# disabled cause not working\n",
    "\n",
    "# with open(f\"{temp_path}ngram_corpus.p\", 'wb') as f:\n",
    "#    pickle.dump(corpus, f)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement d'un modèle word2vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger la liste des n-grams\n",
    "# disabled cause not working\n",
    "\n",
    "# corpus = pickle.load(open(\"ngrams_list.p\", \"rb\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 53s\n",
      "Wall time: 40min 29s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec( # default parameters\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=5, # La taille du \"contexte\" (avant/après le mot)\n",
    "    min_count=5, # On ignore les mots qui n'apparaissent pas au moins 'min_count' fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en plusieurs threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs\n",
    ")\n",
    "\n",
    "# Sauver le modèle dans un fichier\n",
    "outfile = f\"{temp_path}newspapers_window5_mincount5.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 5min 47s\n",
      "Wall time: 34min 51s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec( # smaller window, higher min_count\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=3, # La taille du \"contexte\" (avant/après le mot)\n",
    "    min_count=10, # On ignore les mots qui n'apparaissent pas au moins 'min_count' fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en plusieurs threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs\n",
    ")\n",
    "\n",
    "# Sauver le modèle dans un fichier\n",
    "outfile = f\"{temp_path}newspapers_window3_mincount10.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 6min 27s\n",
      "Wall time: 31min 30s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec( # higher min_count and window\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=10, # La taille du \"contexte\" (avant/après le mot)\n",
    "    min_count=10, # On ignore les mots qui n'apparaissent pas au moins 'min_count' fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en plusieurs threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs\n",
    ")\n",
    "\n",
    "# Sauver le modèle dans un fichier\n",
    "outfile = f\"{temp_path}newspapers_window10_mincount10.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: total: 7min 9s\n",
      "Wall time: 23min 23s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "model = Word2Vec( # higher min_count and window\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=20, # La taille du \"contexte\" (avant/après le mot)\n",
    "    min_count=20, # On ignore les mots qui n'apparaissent pas au moins 'min_count' fois dans le corpus\n",
    "    workers=4, # Permet de paralléliser l'entraînement du modèle en plusieurs threads\n",
    "    epochs=5 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs\n",
    ")\n",
    "\n",
    "# Sauver le modèle dans un fichier\n",
    "outfile = f\"{temp_path}newspapers_window20_mincount20.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration des modèles"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comme proposé dans les consignes du TP, j'ai créé trois modèles jouant sur les paramètres 'min_count' et 'window'. Comme détaillé dans mon rapport, les différences semblent minimes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger les modèles en mémoire\n",
    "\n",
    "models = []\n",
    "\n",
    "models.append(Word2Vec.load(f\"{temp_path}newspapers_window5_mincount5.model\"))\n",
    "models.append(Word2Vec.load(f\"{temp_path}newspapers_window3_mincount10.model\"))\n",
    "models.append(Word2Vec.load(f\"{temp_path}newspapers_window10_mincount10.model\"))\n",
    "models.append(Word2Vec.load(f\"{temp_path}newspapers_window20_mincount20.model\"))\n",
    "\n",
    "nb_models = len(models)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "'journaliste' et 'presse' (modèle 0) : 0.2497072070837021\n",
      "'journaliste' et 'presse' (modèle 1) : 0.23073068261146545\n",
      "'journaliste' et 'presse' (modèle 2) : 0.2886613607406616\n",
      "'journaliste' et 'presse' (modèle 3) : 0.343855082988739\n",
      "\n",
      "'chaud' et 'froid' (modèle 0) : 0.7280045747756958\n",
      "'chaud' et 'froid' (modèle 1) : 0.6839326620101929\n",
      "'chaud' et 'froid' (modèle 2) : 0.7436212301254272\n",
      "'chaud' et 'froid' (modèle 3) : 0.7467522025108337\n",
      "\n",
      "'appartement' et 'louer' (modèle 0) : 0.6763496398925781\n",
      "'appartement' et 'louer' (modèle 1) : 0.6020632982254028\n",
      "'appartement' et 'louer' (modèle 2) : 0.7371500730514526\n",
      "'appartement' et 'louer' (modèle 3) : 0.691918671131134\n",
      "\n",
      "'socialiste' et 'chretien' (modèle 0) : 0.8494765758514404\n",
      "'socialiste' et 'chretien' (modèle 1) : 0.8513742089271545\n",
      "'socialiste' et 'chretien' (modèle 2) : 0.8594279289245605\n",
      "'socialiste' et 'chretien' (modèle 3) : 0.8347303867340088\n",
      "\n",
      "'chanter' et 'citroen' (modèle 0) : -0.16730761528015137\n",
      "'chanter' et 'citroen' (modèle 1) : -0.22362828254699707\n",
      "'chanter' et 'citroen' (modèle 2) : -0.2684934139251709\n",
      "'chanter' et 'citroen' (modèle 3) : -0.22020450234413147\n",
      "\n",
      "'arbre' et 'branche' (modèle 0) : 0.09059195965528488\n",
      "'arbre' et 'branche' (modèle 1) : -0.01615712232887745\n",
      "'arbre' et 'branche' (modèle 2) : 0.03735753148794174\n",
      "'arbre' et 'branche' (modèle 3) : 0.17569226026535034\n",
      "\n",
      "'enseigner' et 'theologiens' (modèle 0) : 0.39570701122283936\n",
      "'enseigner' et 'theologiens' (modèle 1) : 0.3386673033237457\n",
      "'enseigner' et 'theologiens' (modèle 2) : 0.5872240662574768\n",
      "'enseigner' et 'theologiens' (modèle 3) : 0.7725133299827576\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fonction \"similarity\" : Calculer la similarité entre deux termes\n",
    "\n",
    "first_term = [\"journaliste\", \"chaud\", \"appartement\", \"socialiste\", \"chanter\", \"arbre\", \"enseigner\"]\n",
    "second_term = [\"presse\", \"froid\", \"louer\", \"chretien\", \"citroen\", \"branche\", \"theologiens\"]\n",
    "nb_similarity_examples = len(first_term)\n",
    "\n",
    "for i in range (nb_similarity_examples) :\n",
    "    for j in range (nb_models):\n",
    "        print(f\"'{first_term[i]}' et '{second_term[i]}' (modèle {j}) : {models[j].wv.similarity(first_term[i], second_term[i])}\")\n",
    "    print(\"\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mots les plus semblables à 'peugeot' (modèle 0) : [('imperia', 0.9286518692970276), ('citroen', 0.9228317141532898), ('ford', 0.9175989031791687), ('opel', 0.9095059037208557), ('buick', 0.9083935618400574), ('minerva', 0.9041520357131958), ('skoda', 0.9018157124519348), ('chevrolet', 0.9011731147766113), ('mercedes', 0.9002424478530884), ('simca', 0.8955201506614685)]\n",
      "Mots les plus semblables à 'peugeot' (modèle 1) : [('imperia', 0.9339192509651184), ('chrysler', 0.9255465269088745), ('citroen', 0.9190285801887512), ('simca', 0.9189803004264832), ('volkswagen', 0.918897807598114), ('ford', 0.9178043007850647), ('taunus', 0.9104763865470886), ('buick', 0.910352349281311), ('cadillac', 0.9095398783683777), ('opel', 0.9092620611190796)]\n",
      "Mots les plus semblables à 'peugeot' (modèle 2) : [('citroen', 0.9472780227661133), ('ford', 0.9429683089256287), ('chevrolet', 0.9148492813110352), ('imperia', 0.9125781059265137), ('minerva', 0.9059906005859375), ('opel', 0.9050105810165405), ('trlumph', 0.9036298990249634), ('skoda', 0.902930498123169), ('renault', 0.9012725949287415), ('sunbeam', 0.8999972939491272)]\n",
      "Mots les plus semblables à 'peugeot' (modèle 3) : [('citroen', 0.9476141929626465), ('ford', 0.9374614953994751), ('opel', 0.9238443970680237), ('imperia', 0.9179946780204773), ('minerva', 0.917844831943512), ('skoda', 0.9156063199043274), ('daimler', 0.9095557928085327), ('16z', 0.9072868824005127), ('austin', 0.9066075086593628), ('panhard', 0.9061180353164673)]\n",
      "\n",
      "Mots les plus semblables à 'enseigner' (modèle 0) : [('apprendre', 0.852080225944519), ('initier', 0.8154585361480713), ('interpreter', 0.8119053244590759), ('connaitre', 0.7966497540473938), ('traduire', 0.7854831218719482), ('devenir', 0.7852685451507568), ('correspondre', 0.7831083536148071), ('apprecier', 0.779600203037262), ('posseder', 0.7789651155471802), ('plaire', 0.774462878704071)]\n",
      "Mots les plus semblables à 'enseigner' (modèle 1) : [('initier', 0.8820177912712097), ('apprendre', 0.8796849846839905), ('perfectionner', 0.8217111229896545), ('imiter', 0.8176787495613098), ('encourager', 0.8163919448852539), ('illustrer', 0.8131712079048157), ('aider', 0.8130469918251038), ('traduire', 0.8080342411994934), ('nuire', 0.8049202561378479), ('honorer', 0.8042148947715759)]\n",
      "Mots les plus semblables à 'enseigner' (modèle 2) : [('initier', 0.822496235370636), ('apprendre', 0.7751187086105347), ('educateurs', 0.7574115991592407), ('connaissances', 0.7525218725204468), ('enseignent', 0.7448443174362183), ('ambitieux', 0.7419015169143677), ('gouts', 0.7341863512992859), ('inspirer', 0.7319701910018921), ('devenir', 0.7301353216171265), ('talents', 0.7295518517494202)]\n",
      "Mots les plus semblables à 'enseigner' (modèle 3) : [('connaissances', 0.7840512990951538), ('instruits', 0.7767699360847473), ('instruire', 0.7727195620536804), ('theologiens', 0.7725133299827576), ('instruites', 0.7684577107429504), ('enseignent', 0.7650907039642334), ('ambitieux', 0.7570417523384094), ('manuels', 0.7552089691162109), ('initier', 0.7548221945762634), ('educateurs', 0.7546593546867371)]\n",
      "\n",
      "Mots les plus semblables à 'rome' (modèle 0) : [('berlin', 0.9515194296836853), ('moscou', 0.9402503371238708), ('tokio', 0.9336305856704712), ('teheran', 0.9315158724784851), ('londres', 0.9298455715179443), ('madrid', 0.9162214398384094), ('budapest', 0.9018068909645081), ('prague', 0.8976050019264221), ('tanger', 0.8947610259056091), ('geneve', 0.8886193633079529)]\n",
      "Mots les plus semblables à 'rome' (modèle 1) : [('moscou', 0.9416888356208801), ('berlin', 0.9413222074508667), ('londres', 0.9400483965873718), ('tokio', 0.934232771396637), ('budapest', 0.8900594711303711), ('geneve', 0.8881068825721741), ('tanger', 0.8876295685768127), ('teheran', 0.8860889077186584), ('paris', 0.8847982287406921), ('pekin', 0.8838266134262085)]\n",
      "Mots les plus semblables à 'rome' (modèle 2) : [('moscou', 0.9287692904472351), ('teheran', 0.9096048474311829), ('madrid', 0.9062657356262207), ('berlin', 0.90192049741745), ('vienne', 0.8930796980857849), ('beyrouth', 0.8925501108169556), ('londres', 0.8880447149276733), ('stockholm', 0.8795685172080994), ('tokio', 0.878065824508667), ('ankara', 0.8765305280685425)]\n",
      "Mots les plus semblables à 'rome' (modèle 3) : [('moscou', 0.9038795232772827), ('vienne', 0.8942204713821411), ('berlin', 0.8851866722106934), ('madrid', 0.8809231519699097), ('prague', 0.871940553188324), ('athenes', 0.86897212266922), ('geneve', 0.8685277700424194), ('budapest', 0.8643081188201904), ('stockholm', 0.864036500453949), ('tokio', 0.8511466979980469)]\n",
      "\n",
      "Mots les plus semblables à 'bruxelles' (modèle 0) : [('bru_xelles', 0.8897848129272461), ('bruxeues', 0.8567502498626709), ('bruxolles', 0.8377657532691956), ('bruxellee', 0.8044881224632263), ('brnxelles', 0.7777356505393982), ('bruxellea', 0.776783287525177), ('louvain', 0.7728472948074341), ('cureghem', 0.7690577507019043), ('bruxellos', 0.7633735537528992), ('siege_social', 0.7615975737571716)]\n",
      "Mots les plus semblables à 'bruxelles' (modèle 1) : [('bru_xelles', 0.9052833318710327), ('bruxeues', 0.867481529712677), ('bruxolles', 0.8603906631469727), ('bruxellee', 0.8109713792800903), ('brnxelles', 0.7942126989364624), ('bruxellos', 0.7941885590553284), ('cureghem', 0.7837891578674316), ('schaerbeek', 0.774604320526123), ('bruxellea', 0.771194577217102), ('louvain', 0.7654491662979126)]\n",
      "Mots les plus semblables à 'bruxelles' (modèle 2) : [('bru_xelles', 0.8860286474227905), ('bruxolles', 0.8618738055229187), ('bruxeues', 0.8548868298530579), ('bruxellee', 0.8209426999092102), ('bruxellos', 0.805243968963623), ('anciennement', 0.7774332165718079), ('bruxellea', 0.7769997119903564), ('brnxelles', 0.774400532245636), ('bruxelle', 0.7635330557823181), ('siege_social', 0.7487875819206238)]\n",
      "Mots les plus semblables à 'bruxelles' (modèle 3) : [('bruxolles', 0.8688458204269409), ('bru_xelles', 0.8301274180412292), ('bruxeues', 0.8219416737556458), ('bruxellos', 0.7761638164520264), ('bruxellee', 0.7683249115943909), ('xelles', 0.756738007068634), ('eruxelles', 0.7562731504440308), ('brnxelles', 0.7484238147735596), ('siege_social', 0.7312514185905457), ('bruxelle', 0.7259671092033386)]\n",
      "\n",
      "Mots les plus semblables à 'jardin' (modèle 0) : [('jardinet', 0.9337811470031738), ('grand_jardin', 0.8961366415023804), ('balcon', 0.8928058743476868), ('jardin_arbore', 0.8743969798088074), ('verger', 0.873807966709137), ('beau_jardin', 0.8568984866142273), ('garage', 0.8549654483795166), ('entree_particuliere', 0.840053915977478), ('potager', 0.8295042514801025), ('confort_moderne', 0.8288912177085876)]\n",
      "Mots les plus semblables à 'jardin' (modèle 1) : [('balcon', 0.9263514280319214), ('jardinet', 0.9191027879714966), ('grand_jardin', 0.894966185092926), ('verger', 0.8729457855224609), ('jardin_arbore', 0.8632520437240601), ('garage', 0.8572667241096497), ('confort_moderne', 0.8521441221237183), ('entree_particuliere', 0.8445857763290405), ('beau_jardin', 0.8441926836967468), ('dependances', 0.8276559114456177)]\n",
      "Mots les plus semblables à 'jardin' (modèle 2) : [('grand_jardin', 0.9232218265533447), ('jardinet', 0.9110967516899109), ('beau_jardin', 0.9002858996391296), ('verger', 0.8710936903953552), ('jardin_arbore', 0.8701909184455872), ('iardin', 0.8581186532974243), ('nombreux_arbres_fruitiers', 0.8427046537399292), ('jardin_potager', 0.8423937559127808), ('dependances', 0.8374006152153015), ('entree_particuliere', 0.8313745856285095)]\n",
      "Mots les plus semblables à 'jardin' (modèle 3) : [('grand_jardin', 0.9246343374252319), ('jardinet', 0.8799678683280945), ('beau_jardin', 0.875268816947937), ('verger', 0.8701554536819458), ('jardin_potager', 0.8613857626914978), ('dependances', 0.8416959643363953), ('confort_moderne', 0.8363227248191833), ('etage', 0.8278181552886963), ('potager', 0.8258340358734131), ('garage', 0.8246409893035889)]\n",
      "\n",
      "Mots les plus semblables à 'habiter' (modèle 0) : [('gerer', 0.7158070802688599), ('installer', 0.6998782753944397), ('se_creer', 0.6857690215110779), ('engager', 0.685725212097168), ('agrandir', 0.6832269430160522), ('resider', 0.6762905716896057), ('occuper', 0.6709827184677124), ('accompagner', 0.6676895618438721), ('visiter', 0.6673272848129272), ('voyager', 0.6648773550987244)]\n",
      "Mots les plus semblables à 'habiter' (modèle 1) : [('se_creer', 0.7893670201301575), ('y_installer', 0.7598313093185425), ('installer', 0.7490921020507812), ('transferer', 0.7470301389694214), ('visiter', 0.7443580031394958), ('gerer', 0.7395171523094177), ('engager', 0.7239821553230286), ('inaugurer', 0.7238685488700867), ('allouer', 0.7215929627418518), ('se_constituer', 0.7167537212371826)]\n",
      "Mots les plus semblables à 'habiter' (modèle 2) : [('gerer', 0.7171013355255127), ('diriger', 0.6941646337509155), ('preference', 0.6876419186592102), ('se_creer', 0.6853224635124207), ('gerante', 0.6726303100585938), ('profession_liberale', 0.6685648560523987), ('est_offerte', 0.6664425730705261), ('resider', 0.6640557050704956), ('officine', 0.6616274118423462), ('convenable', 0.6591019034385681)]\n",
      "Mots les plus semblables à 'habiter' (modèle 3) : [('exploitant', 0.7662962675094604), ('preference', 0.7633079886436462), ('delegue_medical', 0.7628558874130249), ('doit_habiter', 0.7426756620407104), ('gerer', 0.7120507955551147), ('situation_lucrative', 0.710537850856781), ('environs_immediats', 0.7103678584098816), ('officine', 0.7068219184875488), ('similaire', 0.7055140733718872), ('libre_immediatement', 0.6996808052062988)]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Fonction \"most_similar\" : Chercher les mots les plus proches d'un terme donné\n",
    "\n",
    "most_similar_word = [\"peugeot\", \"enseigner\", \"rome\", \"bruxelles\", \"jardin\", \"habiter\"]\n",
    "nb_most_similar_examples = len(most_similar_word)\n",
    "\n",
    "for i in range (nb_most_similar_examples) :\n",
    "    for j in range (nb_models):\n",
    "        print(f\"Mots les plus semblables à '{most_similar_word[i]}' (modèle {j}) :\", models[j].wv.most_similar(most_similar_word[i], topn=10))\n",
    "    print(\"\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c41795ef67e28f443d51e3530ac426b505c5d8c966af08c2f17a523686b1e2cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
