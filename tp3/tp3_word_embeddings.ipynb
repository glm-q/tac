{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP3 : Word2vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "\n",
    "from gensim.models.phrases import Phrases, Phraser\n",
    "from gensim.models import Word2Vec\n",
    "\n",
    "import nltk\n",
    "from nltk.tokenize import wordpunct_tokenize\n",
    "from unidecode import unidecode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Chargement et traitement des phrases du corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création d'un objet qui streame les lignes d'un fichier pour économiser de la RAM\n",
    "\n",
    "class MySentences(object):\n",
    "    \"\"\"Tokenize and Lemmatize sentences\"\"\" #permet de tokenizer des \"doubles\" mots, par ex \"premier\" + \"ministre\" va être considéré comme un token \"premier ministre\"\n",
    "    def __init__(self, filename):\n",
    "        self.filename = filename\n",
    "\n",
    "    def __iter__(self):\n",
    "        for line in open(self.filename, encoding='utf-8', errors=\"backslashreplace\"):\n",
    "            yield [unidecode(w.lower()) for w in wordpunct_tokenize(line)]\n",
    "\n",
    "infile = f\"../data/sents.txt\"\n",
    "sentences = MySentences(infile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des bigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Création de l'objet 'phrases' = \"dictionnaire d'expressions multi-mots associées à un score\", dont les clés correspondent aux termes du corpus\n",
    "\n",
    "bigram_phrases = Phrases(sentences)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Conversion des objets 'phrases' en objet 'phraser' = version light du 'phrases' -> convertit certains unigrams en bigrams s'ils sont pertinents\n",
    "\n",
    "bigram_phraser = Phraser(phrases_model=bigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Détection des trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phrases = Phrases(bigram_phraser[sentences])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "trigram_phraser = Phraser(phrases_model=trigram_phrases)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Créations d'un corpus d'unigrams, bigrams, trigrams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "corpus = list(trigram_phraser[bigram_phraser[sentences]])\n",
    "\n",
    "print(corpus[:100])\n",
    "\n",
    "#imprime une liste de n-grammes, qu'on répère car ils sont séparés par des _\n",
    "\n",
    "#cf. library \"pickle\" pour sauvegarder objet python en format binaire"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Entraînement d'un modèle word2vec sur ce corpus"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "model = Word2Vec(\n",
    "    corpus, # On passe le corpus de ngrams que nous venons de créer\n",
    "    vector_size=32, # Le nombre de dimensions dans lesquelles le contexte des mots devra être réduit, aka. vector_size\n",
    "    window=1, # La taille du \"contexte\", ici 5 mots avant et après le mot observé\n",
    "    min_count=10, # On ignore les mots qui n'apparaissent pas au moins 5 fois dans le corpus\n",
    "    workers=2, # Permet de paralléliser l'entraînement du modèle en 4 threads -> si la machine est sur le point d'exploser, diminuer\n",
    "    epochs=2 # Nombre d'itérations du réseau de neurones sur le jeu de données pour ajuster les paramètres avec la descente de gradient, aka. epochs.\n",
    "\n",
    "# pour tester: petite fenêtre de mots, peu de workers, haut min_count, peu d'epoch -> puis améliorer itérativement le modèle\n",
    "\n",
    ")\n",
    "\n",
    "# Sauver le modèle dans un fichier\n",
    "outfile = f\"../data/newspapers_window1_mincount10_epochs2.model\"\n",
    "model.save(outfile)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploration du modèle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le modèle en mémoire\n",
    "\n",
    "model = Word2Vec.load(\"../data/newspapers.model\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le vecteur d'un terme\n",
    "\n",
    "model.wv[\"ministre\"] #si erreur, vérifier si le mot apparaît bien dans la sélection (surtout si on garde juste des mots très fréquents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Calculer la similarité entre deux termes\n",
    "\n",
    "model.wv.similarity(\"ministre\", \"roi\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Chercher les mots les plus proches d'un terme donné\n",
    "\n",
    "model.wv.most_similar(\"ministre\", topn=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Faire des recherches complexes à travers l'espace vectoriel\n",
    "\n",
    "print(model.wv.most_similar(positive=['paris', 'londres'], negative=['belgique'])) #positive = proche, negative=éloigné"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "c41795ef67e28f443d51e3530ac426b505c5d8c966af08c2f17a523686b1e2cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
