{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# TP2 : Extraction d'information"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Keywords\n",
    "import os\n",
    "import yake\n",
    "\n",
    "# Word Cloud\n",
    "from collections import Counter\n",
    "from wordcloud import WordCloud\n",
    "import nltk\n",
    "nltk.download('stopwords')\n",
    "from nltk.corpus import stopwords\n",
    "from IPython.display import Image\n",
    "\n",
    "# NER\n",
    "from collections import defaultdict\n",
    "import sys\n",
    "import spacy\n",
    "from spacy.lang.fr.examples import sentences\n",
    "nlp = spacy.load('fr_core_news_md')\n",
    "\n",
    "# Sentiment Analysis\n",
    "from textblob import Blobber\n",
    "from textblob_fr import PatternTagger, PatternAnalyzer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Création d'un fichier avec tous les journaux de l'année 1912"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "year = 1912\n",
    "\n",
    "# Lister les fichiers de cette année\n",
    "data_path = '../data'\n",
    "txt_path = '../data/txt'\n",
    "txts = [f for f in os.listdir(txt_path) if os.path.isfile(os.path.join(txt_path, f)) and str(year) in f]\n",
    "\n",
    "# Stocker le contenu de ces fichiers dans une liste\n",
    "content_list = []\n",
    "for txt in txts:\n",
    "    with open(os.path.join(txt_path, txt), 'r', encoding=\"utf-8\") as f:\n",
    "        content_list.append(f.read())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On peut compter le nombre d'éléments (= fichiers) dans la liste -> c'est (presque) toujours 100, ça permet donc de vérifier s'il n'y a pas eu de problème dans l'exécution du code\n",
    "print(len(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ecrire tout le contenu dans un fichier temporaire\n",
    "temp_path = '../data/tmp'\n",
    "if not os.path.exists(temp_path):\n",
    "    os.mkdir(temp_path)\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'w', encoding=\"utf-8\") as f:\n",
    "    f.write(' '.join(content_list))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer le contenu du fichier et constater les \"déchets\"\n",
    "with open(os.path.join(temp_path, f'{year}.txt'), 'r', encoding=\"utf-8\") as f:\n",
    "    file_year = f.read()\n",
    "\n",
    "file_year[:500]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extraction de mots clés avec Yake"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Instantier l'extracteur de mots clés en fr + les 50 premiers\n",
    "kw_extractor = yake.KeywordExtractor(lan=\"fr\", top=50)\n",
    "kw_extractor\n",
    "\n",
    "# Récupérer le texte du temporaire de l'année étudiée, en mode lecture ('r'), en encodage utf-8\n",
    "text = open(os.path.join(temp_path, f\"{year}.txt\"), 'r', encoding=\"utf-8\").read()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Extraire les mots clés de ce texte\n",
    "keywords = kw_extractor.extract_keywords(text)\n",
    "keywords"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ne garder que les bigrammes / trigrammes et plus\n",
    "kept = []\n",
    "for kw, score in keywords:\n",
    "    words = kw.split()\n",
    "    if len(words) == 2:\n",
    "    # if len(words) == 3:\n",
    "    # if len(words) > 3 :\n",
    "        kept.append(kw)\n",
    "kept"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Nuage de mots"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Stopwords\n",
    "sw = stopwords.words(\"french\")\n",
    "sw += [\n",
    "    # Coquilles OCR pas enlevées par fonction de nettoyage\n",
    "    \"dem\", \"serv\", \"chaus\", \"aujourd\", \"sach\", \"jard\",\"brux\",\"aven\", \"occas\", \"pens\", \"repr\", \"prés\", \"chamb\", \"culs\",\n",
    "    # Mots (quasiment) vides de sens\n",
    "    \"les\", \"plus\", \"cette\", \"fait\", \"faire\", \"être\", \"deux\", \"comme\", \"dont\", \"tout\", \"ils\", \"bien\", \"sans\", \"peut\", \"tous\", \"après\", \"ainsi\", \"donc\", \"cet\", \"sous\", \"celle\", \"entre\", \"encore\", \"toutes\", \"toute\", \"pendant\", \"moins\", \"dire\", \"voir\", \"cela\", \"non\", \"faut\", \"trois\", \"quatre\", \"cinq\", \"quart\", \"demi\", \"aussi\", \"dit\", \"avoir\", \"doit\", \"contre\", \"depuis\", \"autres\", \"van\", \"het\", \"autre\", \"jusqu\", \"très\", \"trop\", \"chez\", \"près\", \"toutes\", \"leurs\", \"avant\", \"suite\", \"rien\", \"quelques\", \"puis\", \"alors\", \"quand\", \"ceux\", \"elles\", \"déjà\", \"celui\", \"devant\", \"toujours\", \"outre\", \"tant\", \"mieux\", \"assez\", \"beaucoup\", \"plusieurs\", \"quelque\", \"quelques\", \"vers\",\n",
    "    # Mots pas pertinents dans le contexte du journal\n",
    "    \"rossel\", \"agence\", \"nord\", \"midi\", \"royale\", \"ville\", \"avenue\", \"place\", \"boulevard\", \"chaussée\", \"saint\", \"octobre\", \"mardi\", \"août\", \"dimanche\", \"septembre\", \"lundi\", \"décembre\", \"janvier\", \"juin\", \"avril\", \"mercredi\", \"samedi\", \"novembre\", \"jeudi\", \"vendredi\"]\n",
    "sw = set(sw)\n",
    "\n",
    "# Fonction de nettoyage\n",
    "\n",
    "def clean_text(year, folder=None):\n",
    "    if folder is None:\n",
    "        input_path = f\"{year}.txt\"\n",
    "        output_path = f\"{year}_clean.txt\"\n",
    "    else:\n",
    "        input_path = f\"{folder}/{year}.txt\"\n",
    "        output_path = f\"{folder}/{year}_clean.txt\"\n",
    "    output = open(output_path, \"w\", encoding='utf-8')\n",
    "    with open(input_path, encoding='utf-8') as f:\n",
    "        text = f.read()\n",
    "        words = nltk.wordpunct_tokenize(text)\n",
    "        kept = [w.lower() for w in words if len(w) > 3 and w.isalpha() and w.lower() not in sw]\n",
    "        kept_string = \" \".join(kept)\n",
    "        output.write(kept_string)\n",
    "    return f'Output has been written in {output_path}!'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Appliquer la fonction et vérifier les résultats\n",
    "\n",
    "clean_text(year, folder=temp_path)\n",
    "\n",
    "with open(os.path.join(temp_path, f'{year}_clean.txt'), 'r', encoding=\"utf-8\") as f:\n",
    "    after = f.read()\n",
    "\n",
    "# print(after[:500], \"\\n---\")\n",
    "\n",
    "# Calculer les mots les plus fréquents et afficher les résultats\n",
    "\n",
    "frequencies = Counter(after.split())\n",
    "\n",
    "print(frequencies.most_common(200))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Créer, stocker, afficher le nuage de mots\n",
    "\n",
    "cloud = WordCloud(width=2000, height=1000, background_color='white').generate_from_frequencies(frequencies)\n",
    "cloud.to_file(os.path.join(temp_path, f\"{year}.png\"))\n",
    "Image(filename=os.path.join(temp_path, f\"{year}.png\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reconnaissance d'entités nommées avec SpaCy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Charger le texte\n",
    "n=100000\n",
    "text_NER = open(os.path.join(temp_path, f\"{year}.txt\"), 'r', encoding=\"utf-8\").read()[:n]\n",
    "\n",
    "# Traiter le texte\n",
    "\n",
    "doc = nlp(text_NER)\n",
    "\n",
    "# Compter les entités (personnes, lieux, organisations)\n",
    "\n",
    "people = defaultdict(int)\n",
    "places = defaultdict(int)\n",
    "orga = defaultdict(int)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    if ent.label_ == \"PER\" and len(ent.text) > 3:\n",
    "        people[ent.text] += 1\n",
    "    elif ent.label_ == \"LOC\" and len(ent.text) > 3:\n",
    "        places[ent.text] += 1\n",
    "    elif ent.label_ == \"ORG\" and len(ent.text) > 3:\n",
    "        orga[ent.text] += 1\n",
    "\n",
    "# Trier\n",
    "\n",
    "sorted_people = sorted(people.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_places = sorted(places.items(), key=lambda kv: kv[1], reverse=True)\n",
    "sorted_orga = sorted(orga.items(), key=lambda kv: kv[1], reverse=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer les 50 premières personnes\n",
    "\n",
    "for person, freq in sorted_people[:50]:\n",
    "    print(f\"{person} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer les 50 premiers lieux\n",
    "\n",
    "for place, freq in sorted_places[:50]:\n",
    "    print(f\"{place} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imprimer les 50 premières organisations\n",
    "\n",
    "for orga, freq in sorted_orga[:50]:\n",
    "    print(f\"{orga} apparait {freq} fois dans le corpus\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sentiment analysis avec Textblob-FR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Fonction\n",
    "\n",
    "tb = Blobber(pos_tagger=PatternTagger(), analyzer=PatternAnalyzer())\n",
    "\n",
    "def get_sentiment(input_text):\n",
    "    blob = tb(input_text)\n",
    "    polarity, subjectivity = blob.sentiment\n",
    "    polarity_perc = f\"{100*abs(polarity):.0f}\"\n",
    "    subjectivity_perc = f\"{100*subjectivity:.0f}\"\n",
    "    if polarity > 0:\n",
    "        polarity_str = f\"{polarity_perc}% positive\"\n",
    "    elif polarity < 0:\n",
    "        polarity_str = f\"{polarity_perc}% negative\"\n",
    "    else:\n",
    "        polarity_str = \"neutral\"\n",
    "    if subjectivity > 0:\n",
    "        subjectivity_str = f\"{subjectivity}% subjective\"\n",
    "    else:\n",
    "        subjectivity_str = \"perfectly objective\"\n",
    "    print(f\"This text is {polarity_str} and {subjectivity_str}.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Exécuter la fonction sur des phrases choisies manuellement\n",
    "\n",
    "get_sentiment(\"Trois d’entre elles purent être assez rapidement dégagées; quant au jeune Taelemans il fut tiré de sa situation critique sans connaissance.\")\n",
    "get_sentiment(\"Les champs sont sous eau, et tes fermiers repêchent ce qu’ils peuvent sauver du désastre.\")\n",
    "get_sentiment(\"La Fédération des jeunes-gardes libérales organise un congrès extraordinaire qui aura lieu le 24 novembre prochain, en vue de discuter la question de la révision.\")\n",
    "get_sentiment(\"Les coups de revolver et de couteau devaient faire une troisième victime.\")\n",
    "get_sentiment(\"Le Roi est en grande tenue de lieutenant- général, la Reine porte une merveilleuse toilette recouverte de véritable dentelle, ainsi que le manteau do dentelle qui lui fut offert par les •dames belges Jars de son mariage, et représentant les écuesons des neuf provlnces.\")\n",
    "get_sentiment(\"Voyant ses maigres ressources- épuisées, cl redoutant pardessus tout de retourner A lu colonie, le malheureux s’est suicidé hier soir en se pendant A la poignée de sa porte On n trouvé sur sa table un billet dans lequel U exposait, en quelques mots, les motifs do sa funeste détermination.\")\n",
    "get_sentiment(\"La presse allemande La limette de Yoss, ce matin, célèbre l'héroïsme du capitaine du Titanic, et considère que. 1e récit de sa mort est une belle page d’enseignement à lire aux enfants.\")\n",
    "get_sentiment(\"Si vous avez une gastrite, une dyspepsie, une gastralgie, une dilatation, une entérite, des indigestions,des vomissements; si après le repas vous avez des renvois, des lourdeurs, des aigreurs, des gonflements, des sur- focations.despalpitations, des maux de tête, de la somnolence; si vous ôtes devenu triste, mélancolique, neurasthénique, prenez les Poudres de Gock.\")\n",
    "get_sentiment(\"Mais, boudeuse, Lili lui refusait, sa main ' pour se cramponner au bras de Grenoult.\")\n",
    "get_sentiment(\"Par suite relations insuf. on dés. marier j. fille 22 a , jolie, bon. éducat., enfant unique, mère veuve, dot 150,000 fr.. plus bel. eapér- à M. dlstine- poslt. sfalsle, préfér. industr. Discrétion. Itettr. signées.\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.10.5 ('tac_venv': venv)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "c41795ef67e28f443d51e3530ac426b505c5d8c966af08c2f17a523686b1e2cc"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
